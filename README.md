# Towards Self-Reflecting Deep Reinforcement Learning Agents

Deep Reinforcement Learning (DRL) algorithms are commonly evaluated with respect to their ability to maximize some
notion of a task related cumulative reward and to generalize this ability across multiple tasks while their
computational demand of developing this ability should be as low a possible. Despite the success of DRL applications
across a variety of tasks, DRL agents usually lack the ability to reflect on their behavior during learning on a
scale other than the one given by a fixed reward function. In this paper, we address this lack of self-reflection
and follow an approach to enable an agent to adapt its reward function based on the development of its learned policy 
during training. We  demonstrate the approach revisiting the well-trodden paths of the cart pole swing up scenario and 
show, that the incorporation of self-reflection reduces the training time and stabilizes learning. We argue that, 
similar to humans, self-reflection is an important concept for agents to improve upon the problem of sample 
inefficiency in the DRL domain.
 
## Citation

```
@article{meyes2018towards,
  title={Towards Self-Reflecting Deep Reinforcement Learning Agents},
  author={Meyes*, Richard and Saner*, Deniz and Meisen, Tobias},
  journal={?},
  year={2018},
  url={?}
}
```
